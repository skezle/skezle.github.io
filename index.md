---
layout: default
---

![Sam Kessler](https://raw.github.com/skezle/skezle.github.io/master/_assets/me.png "me")

* I'm joining Microsoft Cambridge as a Post-doc.
* I did my PhD student in the **Machine Learning Research Group** under
 the supervision of [Steve Roberts](https://www.robots.ox.ac.uk/~sjrob/). My thesis is entitled "Retaining skills under distribution shifts: Bayesian continual learning, continual reinforcement learning, and applications" (to be made public soon).

# News

* I am joining Microsoft Cambridge as a Post-doc thinking about LLM efficiency; how to make the LLM inference and training more efficient.
* I am reviewing for Neurips 24.

# Publications
### Preprints
* **Kessler, Samuel**\*, Le, Tam\*, Nguyen, Vu. "SAVA: Scalable Learning-Agnostic Data Valuation", under submission \[[arXiv](https://arxiv.org/pdf/2406.01130)], \[[code](https://github.com/skezle/sava)].
* Davis, Oscar, **Kessler, Samuel**, et al. "Fisher Flow Matching for Generative Modeling over Discrete Data", under submission \[[arXiv](https://arxiv.org/pdf/2405.14664)].


### Published papers
* **Kessler, Samuel**, et al. "The Effectiveness of World Models for Continual Reinforcement Learning", in CoLLAs 2023 \[[arXiv](https://arxiv.org/abs/2211.15944)\], \[[code](https://github.com/skezle/continual-dreamer)\].  
Also appeared at the Deep Reinforcement Learning workshop @ Neurips 2022.

* Woods, Kieran\* and **Kessler, Samuel**\*, et al. "Few-shot Learning Patterns in Financial Time-Series for Trend-Following Strategies", in Journal of Financial Data Science \[[arXiv](http://arxiv.org/abs/2310.10500)\].

* **Kessler, Samuel**, et al. "On Sequential Bayesian Inference for Continual Learning?", in Entropy 2023 \[[Entropy](https://www.mdpi.com/1099-4300/25/6/884)\], \[[code](https://github.com/skezle/bayes4cl)\].  
    Also appeared at the Advances in Approximate Bayesian Inference workshop 2022.

* **Kessler, Samuel** et al. "Same State, Different Task: Continual Reinforcement Learning without Interference", *oral presentation* in AAAI 2022, \[[arXiv](https://arxiv.org/abs/2106.02940)\], \[[code](https://github.com/skezle/owl)\].  
    Continual learning workshop @ ICML, 2020.

* **S. Kessler**, B. Thomas, S. Karout. "Continual-wav2vec2: an Application of Continual Learning for Self-Supervised Automatic Speech Recognition", in ICASSP 2022, \[[arXiv](https://arxiv.org/abs/2107.13530)\].  
Self-supervised learning workshop for reasoning and perception @ ICML 2021.

* B. Thomas, **S. Kessler**, S. Karout. "Efficient Adapter Transfer of Self-supervised Speech Models for Automatic Speech Recognition", in ICASSP 2022, \[[arXiv](http://arxiv.org/abs/2202.03218)\].

* J. Hergueux\*, **S. Kessler**\*. "Follow the Leader: Technical and Inspirational Leadership in Open Source Software", in ACM SIGCHI 2022 \[[arxiv](https://arxiv.org/abs/2203.10871)\].
  
* **Kessler, Samuel**, et al. "Hierarchical Indian Buffet Neural Networks for Bayesian Continual Learning", *spotlight* in UAI 2021, \[[arXiv](https://arxiv.org/abs/1912.02290)\] \[[code](https://github.com/skezle/IBP_BNN)\].  
Neurips 2019 Bayesian Deep Learning Workshop.

### Workshop papers
* **Kessler, Samuel**\* and Salas, Arnold\* et al. "Practical Bayesian Learning of Neural Networks via Adaptive Optimisation Methods", in ICML 2020 Workshop on Uncertainty and Robustness in Deep Learning, \[[arXiv](https://arxiv.org/abs/1811.03679)\], \[[code](https://github.com/skezle/BADAM)\].


\*equal contribution

# Teaching
*  Teaching Assistant HT 2021, Applied Machine Learning, Graduate Course, Oxford Univeristy, Oxford Internet Institute.
*  Teaching Assistant MT 2020, Machine Learning, Graduate Course, Oxford Univeristy, Oxford Internet Institute. 

# Find me
skessler{at}robots{dot}ox{dot}ac{dot}uk/[Google Scholar](https://scholar.google.co.uk/citations?hl=en&user=JmjQPXoAAAAJ)/[GitHub](http://github.com/skezle)/[Twitter](http://twitter.com/SamKezz)/[Linkedin](https://uk.linkedin.com/pub/samuel-kessler/39/aa2/79)/[CV](https://drive.google.com/file/d/10bIGex70kYBBULIp6VwpH6FSgBs_GxVN/view?usp=sharing).